from typing import List, Dict

import os
import torch
import argparse
import numpy as np
import transformers
import matplotlib.pyplot as plt
import seaborn as sns

from tqdm import tqdm
from torch.utils.data import DataLoader
from scipy.stats import ttest_ind, false_discovery_control

from model_utils import get_layer_names, get_hidden_dim
from utils import setup_hooks
from datasets import LangLocDataset, TOMLocDataset, MDLocDataset, MoralFoundationDataset

# To cache the language mask
CACHE_DIR = os.environ.get("LOC_CACHE", f"cache")

def extract_batch(
    model: torch.nn.Module, 
    input_ids: torch.Tensor, 
    attention_mask: torch.Tensor,
    layer_names: List[str],
    pooling: str = "last-token",
):
    
    batch_activations = {layer_name: [] for layer_name in layer_names}
    hooks, layer_representations = setup_hooks(model, layer_names)

    with torch.no_grad():
        _ = model(input_ids=input_ids, attention_mask=attention_mask)

    for sample_idx in range(len(input_ids)):
        for layer_idx, layer_name in enumerate(layer_names):
            if pooling == "mean":
                activations = layer_representations[layer_name][sample_idx].mean(dim=0).cpu()
            elif pooling == "sum":
                activations = layer_representations[layer_name][sample_idx].sum(dim=0).cpu()
            else:
                activations = layer_representations[layer_name][sample_idx][-1].cpu()    
            batch_activations[layer_name] += [activations]

    for hook in hooks:
        hook.remove()

    return batch_activations

def extract_representations(
    network: str,
    pooling: str,
    model: torch.nn.Module,
    tokenizer: transformers.PreTrainedTokenizer,
    layer_names: List[str],
    hidden_dim: int,
    batch_size: int,
    device: torch.device,
    foundation: str = None,
) -> Dict[str, Dict[str, np.array]]:

    if network == "language":
        loc_dataset = LangLocDataset()
    elif network == "theory-of-mind":
        loc_dataset = TOMLocDataset()
    elif network == "multiple-demand":
        loc_dataset = MDLocDataset()
    elif network == "moral":
        loc_dataset = MoralFoundationDataset(foundation=foundation)
    else:
        raise ValueError(f"Unsupported network: {network}")

    # Get the activations of the model on the dataset
    langloc_dataloader = DataLoader(loc_dataset, batch_size=batch_size, num_workers=0)

    print(f"> Using Device: {device}")

    model.eval()
    model.to(device)

    final_layer_representations = {
        "positive": {layer_name: np.zeros((len(loc_dataset.positive), hidden_dim)) for layer_name in layer_names},
        "negative": {layer_name: np.zeros((len(loc_dataset.negative), hidden_dim)) for layer_name in layer_names}
    }
    
    for batch_idx, batch_data in tqdm(enumerate(langloc_dataloader), total=len(langloc_dataloader)):

        sents, non_words = batch_data
        if network == "language":
            sent_tokens = tokenizer(sents, truncation=True, max_length=12, return_tensors='pt').to(device)
            non_words_tokens = tokenizer(non_words, truncation=True, max_length=12, return_tensors='pt').to(device)
        else:
            sent_tokens = tokenizer(sents, padding=True, return_tensors='pt').to(device)
            non_words_tokens = tokenizer(non_words, padding=True, return_tensors='pt').to(device)
        
        batch_real_actv = extract_batch(model, sent_tokens["input_ids"], sent_tokens["attention_mask"], layer_names, pooling)
        batch_rand_actv = extract_batch(model, non_words_tokens["input_ids"], non_words_tokens["attention_mask"], layer_names, pooling)

        for layer_name in layer_names:
            final_layer_representations["positive"][layer_name][batch_idx*batch_size:(batch_idx+1)*batch_size] = torch.stack(batch_real_actv[layer_name]).numpy()
            final_layer_representations["negative"][layer_name][batch_idx*batch_size:(batch_idx+1)*batch_size] = torch.stack(batch_rand_actv[layer_name]).numpy()

    return final_layer_representations

def plot_statistics(t_values_matrix, p_values_matrix, layer_names, foundation, model_id, percentage=1.0, save_dir="plots"):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # Calculate the t-value threshold for top 1%
    num_units = int((percentage/100) * t_values_matrix.size)
    t_threshold = np.sort(np.abs(t_values_matrix.flatten()))[-num_units]
    
    # Plot t-values distribution
    sns.boxplot(data=[t_values_matrix[i] for i in range(len(layer_names))], ax=ax1)
    ax1.set_xticklabels(layer_names, rotation=45)
    ax1.set_title(f'T-values distribution across layers\n{foundation}')
    ax1.set_ylabel('T-values')
    # Add threshold lines for positive and negative t-values
    ax1.axhline(y=t_threshold, color='r', linestyle='--', label=f'Top {percentage}% threshold')
    ax1.axhline(y=-t_threshold, color='r', linestyle='--')
    ax1.legend()
    
    # Plot -log10(p-values) distribution
    log_p_values = -np.log10(p_values_matrix + 1e-300)
    sns.boxplot(data=[log_p_values[i] for i in range(len(layer_names))], ax=ax2)
    ax2.set_xticklabels(layer_names, rotation=45)
    ax2.set_title(f'-log10(p-values) distribution across layers\n{foundation}')
    ax2.set_ylabel('-log10(p-values)')
    ax2.axhline(y=-np.log10(0.05), color='r', linestyle='--', label='p=0.05')
    
    # Add text with statistics
    selected_p_values = p_values_matrix[np.abs(t_values_matrix) >= t_threshold]
    significant_selected = (selected_p_values < 0.05).sum()
    ax2.text(0.02, 0.98, 
             f'Selected units: {num_units}\n'
             f'Significant among selected: {significant_selected} ({significant_selected/len(selected_p_values)*100:.1f}%)',
             transform=ax2.transAxes, verticalalignment='top')
    ax2.legend()
    
    plt.tight_layout()
    save_path = f"{save_dir}/statistics_{model_id}_foundation={foundation}.png"
    plt.savefig(save_path)
    plt.close()

def localize(model_id: str,
    network: str,
    pooling: str,
    model: torch.nn.Module, 
    num_units: int, 
    tokenizer: transformers.PreTrainedTokenizer, 
    hidden_dim: int, 
    layer_names: List[str], 
    batch_size: int,
    seed: int,
    device: torch.device,
    percentage: float = None,
    localize_range: str = None,
    pretrained: bool = True,
    overwrite: bool = False,
    foundation: str = None,
):
    """
    Localize network selective units in the model.
    """

    range_start, range_end = map(int, localize_range.split("-"))

    save_path = f"{CACHE_DIR}/{model_id}_network={network}_foundation={foundation}_pooling={pooling}_range={localize_range}_perc={percentage}_nunits={num_units}_pretrained={pretrained}.npy"
    save_path_pvalues = f"{CACHE_DIR}/{model_id}_network={network}_foundation={foundation}_pooling={pooling}_pretrained={pretrained}_pvalues.npy"

    if os.path.exists(save_path) and not overwrite:
        print(f"> Loading mask from {save_path}")
        return np.load(save_path)

    representations = extract_representations(
        network=network, 
        pooling=pooling,
        model=model, 
        tokenizer=tokenizer, 
        layer_names=layer_names, 
        hidden_dim=hidden_dim, 
        batch_size=batch_size, 
        device=device,
        foundation=foundation,
    )

    p_values_matrix = np.zeros((len(layer_names), hidden_dim))
    t_values_matrix = np.zeros((len(layer_names), hidden_dim))

    for layer_idx, layer_name in tqdm(enumerate(layer_names), total=len(layer_names)):
        positive_actv = np.abs(representations["positive"][layer_name])
        negative_actv = np.abs(representations["negative"][layer_name])
        t_values_matrix[layer_idx], p_values_matrix[layer_idx] = ttest_ind(positive_actv, negative_actv, axis=0, equal_var=False)

        print(f"> Starting statistics plotting...")
        plot_statistics(
            t_values_matrix, 
            p_values_matrix, 
            layer_names, 
            foundation,
            model_id
        )
        print(f"> Finished statistics plotting")

    def is_topk(a, k=1):
        _, rix = np.unique(-a, return_inverse=True)
        return np.where(rix < k, 1, 0).reshape(a.shape)
    
    def is_bottomk(a, k=1):
        _, rix = np.unique(a, return_inverse=True)
        return np.where(rix < k, 1, 0).reshape(a.shape)
    
    np.random.seed(seed)
    if percentage is not None:
        num_units = int((percentage/100) * hidden_dim*len(layer_names))
        print(f"> Percentage: {percentage}% --> Num Units: {num_units}")

    if localize_range is not None and range_start < range_end:
        range_start_val = np.percentile(t_values_matrix, range_start)
        range_end_val = np.percentile(t_values_matrix, range_end)
        # take random num_units from that percentile range
        mask_range = (t_values_matrix >= range_start_val) & (t_values_matrix <= range_end_val)
        total_num_units = np.prod(mask_range.shape)
        mask_range_indices = np.arange(total_num_units)[mask_range.flatten()]
        rand_indices = np.random.choice(mask_range_indices, size=num_units, replace=False)
        language_mask = np.full(total_num_units, 0)
        language_mask[rand_indices] = 1
        language_mask = language_mask.reshape(mask_range.shape)
        print(f"> Num units in range {range_start}-{range_end}: {language_mask.sum()}")
    elif localize_range and range_start == range_end and int(range_start) == 0:
        language_mask = is_bottomk(t_values_matrix, k=num_units)
    else:
        language_mask = is_topk(t_values_matrix, k=num_units)

    print(f"> Num units: {language_mask.sum()}")
    num_layers, num_units = p_values_matrix.shape
    adjusted_p_values = false_discovery_control(p_values_matrix.flatten())
    adjusted_p_values = adjusted_p_values.reshape((num_layers, num_units))

    np.save(save_path, language_mask)
    np.save(save_path_pvalues, adjusted_p_values)
    print(f"> {model_id} {network} mask cached to {save_path}")

    return language_mask

if  __name__ == "__main__":

    parser = argparse.ArgumentParser(description="Localize Units in LLMs")
    parser.add_argument("--model-name", type=str, required=True, help="huggingface model name")
    parser.add_argument("--percentage", type=float, default=None, help="percentage of units to localize")
    parser.add_argument("--localize-range", type=str, default="100-100", help="percentile in which to localize, 100-100 and 0-0 indicate top and least selective units respectively")
    parser.add_argument("--network", type=str, default="language", help="network to localize")
    parser.add_argument("--pooling", type=str, default="last-token", choices=["last-token", "mean"], help="token aggregation method")
    parser.add_argument("--num-units", type=int, default=None, help="number of units to localize, percentage overrides it")
    parser.add_argument("--seed", type=int, default=42, help="random seed")
    parser.add_argument("--device", type=str, default=None, help="device to use")
    parser.add_argument("--untrained", action="store_true", help="use an untrained version of the model")
    parser.add_argument("--overwrite", action="store_true", help="overwrite current mask if cached")
    parser.add_argument("--foundation", type=str, default="care", 
                       choices=["care", "fairness", "loyalty", "authority", "sanctity", "liberty"])
    args = parser.parse_args()

    assert args.percentage or args.num_units, "You must either provide percentage of units to localize or number of units"
    assert args.network in {"language", "theory-of-mind", "multiple-demand", "moral"}, "Unsupported network"

    model_name = args.model_name
    pretrained = not args.untrained
    localize_range = args.localize_range
    num_units = args.num_units
    percentage = args.percentage
    pooling = args.pooling
    network = args.network
    seed = args.seed
    batch_size = 1
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") if args.device is None else args.device

    if pretrained:
        model = transformers.AutoModelForCausalLM.from_pretrained(model_name)
    else:
        model_config = transformers.AutoConfig.from_pretrained(model_name)
        model = transformers.AutoModelForCausalLM.from_config(config=model_config)

    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    model_name = os.path.basename(model_name)

    model_layer_names = get_layer_names(model_name)
    hidden_dim = get_hidden_dim(model_name)

    model.eval()

    localize(
        model_id=model_name,
        network=network,
        pooling=pooling,
        model=model,
        num_units=num_units,
        percentage=percentage,
        tokenizer=tokenizer,
        hidden_dim=hidden_dim,
        layer_names=model_layer_names,
        batch_size=batch_size,
        seed=seed,
        device=device,
        localize_range=localize_range,
        pretrained=pretrained,
        overwrite=args.overwrite,
        foundation=args.foundation,
    )
